# ğŸ¤– Roteirizador de Suporte Inteligente

![Python](https://img.shields.io/badge/Python-3.9-blue?style=for-the-badge&logo=python&logoColor=white)
![Docker](https://img.shields.io/badge/Docker-Enabled-blue?style=for-the-badge&logo=docker&logoColor=white)
![Streamlit](https://img.shields.io/badge/Frontend-Streamlit-red?style=for-the-badge&logo=streamlit&logoColor=white)
![Gemini](https://img.shields.io/badge/AI-Gemini_2.0-orange?style=for-the-badge&logo=google&logoColor=white)

> **Case de Estudo:** Sistema de triagem inteligente aplicando Engenharia de Software e MLOps para otimizar o suporte ao cliente (focado no cenÃ¡rio iFood).

---

## ğŸ“‹ Sobre o Projeto

Este projeto resolve o problema de **triagem manual de tickets de suporte**. Ele utiliza uma abordagem hÃ­brida de InteligÃªncia Artificial para processar reclamaÃ§Ãµes de clientes em tempo real:

1.  **ClassificaÃ§Ã£o de Prioridade (Machine Learning ClÃ¡ssico):** Um modelo Scikit-Learn treinado prevÃª a urgÃªncia do ticket.
2.  **AnÃ¡lise SemÃ¢ntica (Generative AI):** IntegraÃ§Ã£o com **Google Gemini** para resumir o problema e categorizar o ticket (ex: Financeiro, LogÃ­stica, Qualidade).
3.  **Interface de OperaÃ§Ã£o:** Dashboard interativo em Streamlit para o time de atendimento.

---

## ğŸ› ï¸ Arquitetura

O sistema foi desenhado seguindo a arquitetura de MicrosserviÃ§os:

* **Frontend:** Streamlit (Consome a API).
* **Backend:** Flask API (ExpÃµe os modelos de ML e LLM).
* **ContainerizaÃ§Ã£o:** Docker (Garante reprodutibilidade).

---

## ğŸš€ Como Rodar o Projeto

VocÃª pode rodar este projeto de duas formas: via **Docker** (recomendado) ou **Localmente** (Python).

### PrÃ©-requisitos
* Git
* Docker Desktop (para mÃ©todo Docker)
* Python 3.9+ (para mÃ©todo Local)
* Uma chave de API do Google Gemini (Gratuita no Google AI Studio)

### ğŸ” Passo 0: ConfiguraÃ§Ã£o da Chave (ObrigatÃ³rio)

Por seguranÃ§a, as chaves de API nÃ£o ficam no repositÃ³rio.
1.  Na raiz do projeto, crie um arquivo chamado `.env`.
2.  Adicione sua chave do Gemini dentro dele:
    ```env
    GEMINI_API_KEY=sua_chave_aqui_sem_aspas
    ```

---

### ğŸ³ OpÃ§Ã£o A: Rodando com Docker (Recomendado)

O jeito mais fÃ¡cil. O Docker cuida de todas as instalaÃ§Ãµes.

1.  **Construa a imagem:**
    ```bash
    docker build -t roteirizador-ia .
    ```

2.  **Rode o container:**
    (Este comando conecta a porta 5000 e injeta sua chave de API)
    ```bash
    docker run -p 5000:5000 --env-file .env roteirizador-ia
    ```

3.  **Acesse a Interface:**
    O Backend estarÃ¡ rodando. Para ver o Frontend, abra um novo terminal (com Python instalado) e rode:
    ```bash
    pip install streamlit requests
    streamlit run interface.py
    ```

---

### ğŸ OpÃ§Ã£o B: Rodando Localmente (Sem Docker)

1.  **Crie e ative um ambiente virtual:**
    ```bash
    python -m venv .venv
    # Windows:
    .venv\Scripts\activate
    # Linux/Mac:
    source .venv/bin/activate
    ```

2.  **Instale as dependÃªncias:**
    ```bash
    pip install -r requirements.txt
    pip install streamlit requests
    ```

3.  **Inicie os serviÃ§os (Em dois terminais diferentes):**

    * **Terminal 1 (Backend):**
        ```bash
        python app.py
        ```

    * **Terminal 2 (Frontend):**
        ```bash
        streamlit run interface.py
        ```

---

## ğŸ§ª Como Testar

1.  Acesse o endereÃ§o local que o Streamlit mostrarÃ¡ (geralmente `http://localhost:8501`).
2.  Digite um ticket de exemplo na caixa de texto.
    * *Ex: "Meu pedido chegou revirado e frio, quero meu dinheiro de volta."*
3.  Clique em **"Prever Prioridade"** para testar o modelo ML Local.
4.  Clique em **"AnÃ¡lise Profunda"** para testar a integraÃ§Ã£o com o Gemini.

---

## ğŸ“‚ Estrutura de Arquivos

* `app.py`: Servidor API (Flask).
* `interface.py`: Interface do UsuÃ¡rio (Streamlit).
* `Dockerfile`: Receita de construÃ§Ã£o do container.
* `exploracao.ipynb`: Notebook de treino e anÃ¡lise de dados.
* `*.pkl`: Modelos de ML serializados.

---

Desenvolvido por **Fernanda Brito** como parte de estudos avanÃ§ados em Engenharia de ComputaÃ§Ã£o e IA.